{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc83fbe-cc1c-4aba-a546-f91548946024",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "# %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1875e9b-5e2e-493d-8b2b-73d9aa285b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa406ef-91d9-46dd-868f-34997377d09e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67543705-eae9-4990-bcb4-3637b810c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat(\"./cho/s01.mat\", squeeze_me=True, struct_as_record=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6881e29-4b07-48f6-825b-b6c0c080c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noise',\n",
       " 'rest',\n",
       " 'srate',\n",
       " 'movement_left',\n",
       " 'movement_right',\n",
       " 'movement_event',\n",
       " 'n_movement_trials',\n",
       " 'imagery_left',\n",
       " 'imagery_right',\n",
       " 'n_imagery_trials',\n",
       " 'frame',\n",
       " 'imagery_event',\n",
       " 'comment',\n",
       " 'subject',\n",
       " 'bad_trial_indices',\n",
       " 'psenloc',\n",
       " 'senloc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_file[\"eeg\"]._fieldnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a983d6-250c-451d-8301-1abf8b19d969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### rest: \n",
    "resting state with eyes-open condition. resting state was recorded for 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48209bca-cfc4-4b1a-9873-bff5b41e7533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68, 34048), 66.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = mat_file[\"eeg\"].rest\n",
    "sig.shape, sig.shape[1] / mat_file[\"eeg\"].srate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f378572-b949-455c-bdaa-a34fb04674c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### noise:\n",
    "- eye blinking, 5 seconds × 2\n",
    "- eyeball movement up/down, 5 seconds × 2\n",
    "- eyeball movement left/right, 5 seconds × 2\n",
    "- jaw clenching, 5 seconds × 2\n",
    "- head movement left/right, 5 seconds × 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383d2ab9-7b3c-4fc7-a47a-368231322637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68, 5120), 10.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = mat_file[\"eeg\"].noise[0]\n",
    "sig.shape, sig.shape[1] / mat_file[\"eeg\"].srate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdc255-2041-405b-9cdf-c88c5c6396ae",
   "metadata": {},
   "source": [
    "#### imagery left: \n",
    "100 or 120 trials of left hand MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c92d688-db3e-4942-a513-bca45eabb187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68, 358400), 700.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = mat_file[\"eeg\"].imagery_left\n",
    "sig.shape, sig.shape[1] / mat_file[\"eeg\"].srate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20347d35-f47e-4de7-b3f6-27968e4d25b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### imagery right: \n",
    "100 or 120 trials of right hand MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ee71e4-39f6-4541-be4d-5b5ae4e4be64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68, 358400), 700.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = mat_file[\"eeg\"].imagery_right\n",
    "sig.shape, sig.shape[1] / mat_file[\"eeg\"].srate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fc0f6-2106-4e45-a0f1-bf4328f95dbf",
   "metadata": {},
   "source": [
    "#### senloc & psenloc: \n",
    "3D sensor locations \\\n",
    "sensor location projected to unit sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e959d2-5689-4240-98bc-ba6dea4f5da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 3), (64, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_file[\"eeg\"].senloc.shape, mat_file[\"eeg\"].psenloc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be2bd4-a77c-469c-aa42-36331ac5bf63",
   "metadata": {},
   "source": [
    "### Getting trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d52274-336d-4e70-8107-39b504c22647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012\n"
     ]
    }
   ],
   "source": [
    "print(f\"{12:0=3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5995470-14f7-4c0a-9c5d-6e89248a7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "{3584}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 68, 3584)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(mat_file[\"eeg\"].imagery_event == 1)[0]\n",
    "assert len(ind) == mat_file[\"eeg\"].n_imagery_trials\n",
    "\n",
    "# np.array(ind).inse\n",
    "trial_ranges = [ind[i + 1] - ind[i] for i in range(len(ind) - 1)]\n",
    "print(len(trial_ranges))\n",
    "print(set(trial_ranges))\n",
    "\n",
    "trials = np.vstack(\n",
    "    [\n",
    "        np.expand_dims(mat_file[\"eeg\"].imagery_left[:, ind[i] : ind[i + 1]], axis=0)\n",
    "        for i in range(0, len(ind) - 1)\n",
    "    ]\n",
    ")\n",
    "trials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf7859a3-bc05-4c4c-9761-12916ae03ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([np.full((10, 2), 2), np.zeros((14, 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e594470f-c1a9-4225-802f-0f6b4bc3a739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.zeros(10), np.zeros(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487020d-3274-47b5-8ee1-db51ea4f3c17",
   "metadata": {},
   "source": [
    "## Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed012486-14bc-416f-b328-87df935647ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RepeatedStratifiedKFold\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d9159-2371-49fd-b604-8e133f354473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChoDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        patients=list(range(52)),\n",
    "        paradigm=\"imagery\",  # ToDo: implement 'real' movement\n",
    "        transforms=None,\n",
    "        data: dict = None,\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.patients = patients\n",
    "        self.paradigm = paradigm\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if data is not None:\n",
    "            self.signals = data[\"signals\"]\n",
    "            self.labels = data[\"labels\"]\n",
    "            return\n",
    "\n",
    "        self._load_dataset()\n",
    "\n",
    "    # def get_sampling_rate(self):\n",
    "    #     return 250\n",
    "\n",
    "    # def get_resampling_rate(self):\n",
    "    #     return 256\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        trials_list = []\n",
    "        labels_list = []\n",
    "        for i in self.patients:\n",
    "            mat_file = self._load_patient(i)\n",
    "            patiant_trials, patient_labels = self._extract_imagery(mat_file)\n",
    "            trials_list.append(patiant_trials)\n",
    "            labels_list.append(patient_labels)\n",
    "\n",
    "        self.signals = np.vstack(trials_list)\n",
    "        self.labels = np.hstack(labels_list)\n",
    "\n",
    "    def _extract_imagery(self, mat_file):\n",
    "        ind = np.where(mat_file[\"eeg\"].imagery_event == 1)[0]\n",
    "        assert len(ind) == mat_file[\"eeg\"].n_imagery_trials\n",
    "\n",
    "        trials_left = np.vstack(\n",
    "            [\n",
    "                np.expand_dims(\n",
    "                    mat_file[\"eeg\"].imagery_left[:, ind[i] : ind[i + 1]], axis=0\n",
    "                )\n",
    "                for i in range(0, len(ind) - 1)\n",
    "            ]\n",
    "        )\n",
    "        trials_right = np.vstack(\n",
    "            [\n",
    "                np.expand_dims(\n",
    "                    mat_file[\"eeg\"].imagery_right[:, ind[i] : ind[i + 1]], axis=0\n",
    "                )\n",
    "                for i in range(0, len(ind) - 1)\n",
    "            ]\n",
    "        )\n",
    "        trials = np.vstack([trials_left, trials_right])\n",
    "        labels = np.hstack(\n",
    "            [np.full(len(trials_left), 1), np.full(len(trials_right), 2)]\n",
    "        )\n",
    "\n",
    "        return trials, labels\n",
    "\n",
    "    def _load_patient(self, idx):\n",
    "        return loadmat(\n",
    "            f\"{self.data_path}/s{idx:0=2}.mat\", squeeze_me=True, struct_as_record=False\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav, label = self.signals[idx], self.labels[idx]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            wav, label = self.transforms(wav, label)\n",
    "\n",
    "        return wav, label\n",
    "\n",
    "    def subset(self, indices):\n",
    "        return self.__class__(\n",
    "            data_path=self.data_path,\n",
    "            patients=self.patients,\n",
    "            paradigm=self.paradigm,\n",
    "            transforms=self.transforms,\n",
    "            data={\"signals\": self.signals[indices], \"labels\": self.labels[indices]},\n",
    "        )\n",
    "\n",
    "    def _repeated_kfold_splits(self):\n",
    "        n_splits = 5\n",
    "        n_repeats = 10\n",
    "        persist_path = f\"./CHO_dataset_{n_splits}_splits_{n_repeats}_repeats.json\"\n",
    "\n",
    "        if os.path.exists(persist_path):\n",
    "            with open(persist_path, \"r\") as f:\n",
    "                splits = json.load(f)\n",
    "        else:\n",
    "            cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "            splits = {\n",
    "                f\"split_{i}\": {\"train\": split[0].tolist(), \"test\": split[1].tolist()}\n",
    "                for i, split in enumerate(list(cv.split(self.signals, self.labels)))\n",
    "            }\n",
    "            with open(persist_path, \"w\") as f:\n",
    "                json.dump(splits, f)\n",
    "\n",
    "        self.splits = splits\n",
    "        for key, split in splits.items():\n",
    "            yield key, split[\"train\"], split[\"test\"]\n",
    "\n",
    "    def get_train_test_subsets(self, with_key=False):\n",
    "        for key, train, val in self._repeated_kfold_splits():\n",
    "            if with_key:\n",
    "                yield key, self.subset(train), self.subset(val)\n",
    "            else:\n",
    "                yield self.subset(train), self.subset(val)\n",
    "\n",
    "    @staticmethod\n",
    "    def dict_to_2d_wave(dict_signals):\n",
    "        return np.vstack([wav for wav in dict_signals.values()])\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        imgs = torch.vstack([item[0] for item in batch])\n",
    "\n",
    "        trgts = {}\n",
    "        sample_item_label = batch[0][1]\n",
    "        for label_key in sample_item_label.keys():\n",
    "            if isinstance(sample_item_label[label_key], dict):\n",
    "                trgts[label_key] = {\n",
    "                    key: torch.vstack(\n",
    "                        [item[1][label_key][key].squeeze() for item in batch]\n",
    "                    )\n",
    "                    for key in sample_item_label[label_key].keys()\n",
    "                }\n",
    "            else:\n",
    "                trgts[label_key] = torch.vstack(\n",
    "                    [item[1][label_key] for item in batch]\n",
    "                ).squeeze()\n",
    "\n",
    "        return [imgs, trgts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd9535-b484-445c-881f-8a21e3cab0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ChoDataset(data_path=\"./cho/\", patients=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354aa1ab-0121-43a1-96d9-713399f0afa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, train, test in ds.get_train_test_subsets(with_key=True):\n",
    "    print(key, len(train), len(test))\n",
    "    idx = np.random.randint(0, len(train))\n",
    "    print(train[idx][0].shape, train[idx][1])\n",
    "    # train and evaluate model here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a7d8b-b8a5-4a1c-9f3e-8a121b291a65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652807a2-0f7d-4d71-9f3f-ca2653a697d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any, Union, Callable\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.fabric.utilities.apply_func import apply_to_collection\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ccdf0-6700-42a9-9f7a-b8f90a6c3ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __init__(self, device):\n",
    "        if isinstance(device, str):\n",
    "            device = torch.device(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, data, label):\n",
    "        data = apply_to_collection(\n",
    "            data,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: torch.from_numpy(a),\n",
    "        )\n",
    "        label = apply_to_collection(\n",
    "            label,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: torch.tensor(a, dtype=torch.float64),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "\n",
    "class ToNumpy:\n",
    "    def __call__(self, data, label):\n",
    "        data = apply_to_collection(\n",
    "            data,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: a.cpu().detach().numpy(),\n",
    "        )\n",
    "        label = apply_to_collection(\n",
    "            label,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: a.cpu().detach().numpy(),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "\n",
    "class DictToTensor:\n",
    "    def __call__(self, data: Dict[str, Tensor], label):\n",
    "        # The output shape [batch, channel, signal]\n",
    "        return (\n",
    "            torch.permute(\n",
    "                torch.vstack(list(map(lambda a: a.unsqueeze(0), data.values()))),\n",
    "                (1, 0, 2),\n",
    "            ),\n",
    "            label,\n",
    "        )\n",
    "\n",
    "\n",
    "class DictToArray:\n",
    "    def __call__(self, data, label):\n",
    "        # The output shape [batch, channel, signal]\n",
    "        return (\n",
    "            np.transpose(\n",
    "                np.vstack(\n",
    "                    list(map(lambda a: np.expand_dims(a, axis=0), data.values()))\n",
    "                ),\n",
    "                (1, 0, 2),\n",
    "            ),\n",
    "            label,\n",
    "        )\n",
    "\n",
    "\n",
    "class Windowing:\n",
    "    def __init__(self, n_segments: int = 5, sample_rate: float = 250.0):\n",
    "        self.n_segments = n_segments\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    # The Output of the signal is [batch, channels, windowed, band_filtered, signal]\n",
    "    def __call__(self, data: Tensor, label):\n",
    "        \"\"\"Takes as input a signal tensor of shape [batch, channels, band_filtered, signal]\n",
    "        and outputs a signal tensor of shape [batch, channels, windowed, band_filtered, signal]\n",
    "        \"\"\"\n",
    "        start, end = 0, data.size()[-1]\n",
    "        step = int((end - start) / self.n_segments)\n",
    "        windows = np.arange(start, end - step, step=step)\n",
    "\n",
    "        if len(windows) == 0:\n",
    "            data = data.unsqueeze(dim=2)\n",
    "            return data, label\n",
    "\n",
    "        windowed_data = torch.permute(\n",
    "            torch.stack(\n",
    "                [data[:, :, :, window : (window + step)] for window in windows], dim=0\n",
    "            ),\n",
    "            (1, 2, 0, 3, 4),\n",
    "        )\n",
    "\n",
    "        return windowed_data, label\n",
    "\n",
    "\n",
    "class Filtering:\n",
    "    def __init__(self, N: int, rs: float, Wns: List[float], bandwidth, fs: float):\n",
    "        self.N = N\n",
    "        self.rs = rs\n",
    "        self.Wns = Wns / (fs / 2)  # Normalize the signals\n",
    "        self.bandwidth = bandwidth / (fs / 2)  # Normalize the signals\n",
    "        self.fs = fs\n",
    "\n",
    "    # The Output of the signal is [batch, channels, band_filtered, signal]\n",
    "    def __call__(self, data, label):\n",
    "        filtered_data = []\n",
    "\n",
    "        for wn in self.Wns:\n",
    "            b, a = scipy.signal.cheby2(\n",
    "                N=self.N,\n",
    "                rs=self.rs,\n",
    "                Wn=[wn, wn + self.bandwidth],\n",
    "                btype=\"bandpass\",\n",
    "                fs=self.fs,\n",
    "            )\n",
    "            filtered_data.append(scipy.signal.filtfilt(b, a, data, axis=-1))\n",
    "\n",
    "        filtered_data = torch.permute(torch.Tensor(filtered_data), (1, 2, 0, 3))\n",
    "\n",
    "        return filtered_data, label\n",
    "\n",
    "\n",
    "class ExpandDim(object):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, data, label):\n",
    "        return data.unsqueeze_(self.dim), label\n",
    "\n",
    "\n",
    "class LabelToDict:\n",
    "    def __call__(self, data, label):\n",
    "        return data, {\"label\": label}\n",
    "\n",
    "\n",
    "class ToNumpy:\n",
    "    def __call__(self, data, label):\n",
    "        return data.cpu().detach().numpy(), label.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: List[Callable]) -> None:\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data: Any, target: Any):\n",
    "        for t in self.transforms:\n",
    "            data, target = t(data, target)\n",
    "        return data, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([c.__class__.__name__ for c in self.transforms])\n",
    "\n",
    "\n",
    "# TODO: complete this part\n",
    "from scipy.signal import cheby2, filtfilt\n",
    "\n",
    "\n",
    "def cheby_bandpass_filter(signal, attenuation, lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = cheby2(order, rs=attenuation, Wn=[low, high], btype=\"band\")\n",
    "    y = filtfilt(b, a, signal, axis=-1)\n",
    "    # print(\"filtered shape \", y.shape)\n",
    "    return y\n",
    "\n",
    "\n",
    "def cheby_bandpass_one_subject(\n",
    "    X, attenuation, lowcut, highcut, fs, interval=None, verbose=True\n",
    "):\n",
    "    temp_epoch_EEG = X.copy()\n",
    "    # print(f\"data shape : {temp_epoch_EEG.shape}\")\n",
    "\n",
    "    if interval is not None:\n",
    "        startband = np.arange(lowcut, highcut, step=interval)\n",
    "\n",
    "        bands = []\n",
    "        for start in startband:\n",
    "            # This will be new key inside the EEG_filtered\n",
    "            band = \"{:02d}_{:02d}\".format(start, start + interval)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Filtering through {} Hz band\".format(band))\n",
    "            # Bandpass filtering\n",
    "            bands.append(\n",
    "                cheby_bandpass_filter(\n",
    "                    temp_epoch_EEG, attenuation, start, start + interval, fs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.vstack(bands)\n",
    "\n",
    "    else:\n",
    "        # This will be new key inside the EEG_filtered\n",
    "        band = \"{:02d}_{:02d}\".format(lowcut, highcut)\n",
    "\n",
    "        return cheby_bandpass_filter(temp_epoch_EEG, attenuation, lowcut, highcut, fs)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class BandPass:\n",
    "    def __init__(self, attenuation, lowcut, highcut, fs, interval=None):\n",
    "        self.attenuation = attenuation\n",
    "        self.lowcut = lowcut\n",
    "        self.highcut = highcut\n",
    "        self.fs = fs\n",
    "        self.interval = interval\n",
    "\n",
    "        self.bandpass_func = partial(\n",
    "            cheby_bandpass_one_subject,\n",
    "            attenuation=self.attenuation,\n",
    "            lowcut=self.lowcut,\n",
    "            highcut=self.highcut,\n",
    "            fs=self.fs,\n",
    "            interval=self.interval,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    # The Output of the signal is [batch, channels, band_filtered, signal]\n",
    "    def __call__(self, data, label):\n",
    "        filtered_data = data = apply_to_collection(\n",
    "            data,\n",
    "            dtype=(np.ndarray, int, float, np.int64, Tensor),\n",
    "            function=self.bandpass_func,\n",
    "        )\n",
    "\n",
    "        filtered_data = np.expand_dims(filtered_data.transpose(1, 0, 2), axis=0)\n",
    "\n",
    "        return filtered_data, label\n",
    "\n",
    "\n",
    "class Whitening:\n",
    "    def __init__(self, data_loader, whitening_method=\"PCA\"):\n",
    "        self.ds = data_loader\n",
    "        self.method = whitening_method\n",
    "\n",
    "        self.W = self._generate_whitening_transformation(self.ds, self.method)\n",
    "\n",
    "    def _generate_whitening_transformation(self, data_loader, whitening_method=\"PCA\"):\n",
    "        \"\"\"extract whitening transformation from data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : torch.dataloader\n",
    "            pytorch data loader\n",
    "        whitening_method : str\n",
    "            one of following values\n",
    "            \"PCA\" for PCA whitening\n",
    "            \"ZCA for ZCA whitening\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            whitening transformation matrix\n",
    "        \"\"\"\n",
    "        # get data\n",
    "        signal = []\n",
    "        for sig, lbl in data_loader:\n",
    "            signal.append(sig)\n",
    "        signal = torch.vstack(signal)\n",
    "\n",
    "        # zero center\n",
    "        x = signal.squeeze()\n",
    "        sig = x.permute(0, 2, 1)\n",
    "        x = torch.mean(sig, axis=1)\n",
    "        x_mean = torch.mean(x, axis=0)\n",
    "\n",
    "        x = sig - x_mean\n",
    "        x_zero_centered = x.permute(0, 2, 1)\n",
    "\n",
    "        # Calculate whitening matrix\n",
    "        x_cov = self._calc_cov(x_zero_centered)\n",
    "\n",
    "        lda, V = torch.linalg.eig(x_cov)\n",
    "        lda, V = lda.real, V.real\n",
    "        if \"PCA\":\n",
    "            whitening_mat = torch.sqrt(torch.inverse(torch.diag(lda))) @ V.T\n",
    "        elif \"ZCA\":\n",
    "            whitening_mat = V @ torch.sqrt(torch.inverse(torch.diag(lda))) @ V.T\n",
    "\n",
    "        return whitening_mat\n",
    "\n",
    "    def _calc_cov(self, EEG_data):\n",
    "        cov = []\n",
    "        for i in range(EEG_data.size()[0]):\n",
    "            cov.append(\n",
    "                EEG_data[i] @ EEG_data[i].T / torch.trace(EEG_data[i] @ EEG_data[i].T)\n",
    "            )\n",
    "\n",
    "        cov = torch.mean(torch.stack(cov), 0)\n",
    "\n",
    "        return cov\n",
    "\n",
    "    def __call__(self, data: Tensor, label):\n",
    "        whitened_data = self.W @ data\n",
    "        return whitened_data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41239d0-0fa1-4f7d-a73c-1678698e3fe7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122febf-1908-4eb5-a9f0-f503a39905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe6638-3e34-41bd-b554-42e045824af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from cspnn.csp_nn import CSP, CSPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20d370-514d-4e0d-8540-750cede9fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e1188-0a2d-47c7-a853-11b98d0bd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3f75a-0fc2-4c69-bfaf-1cdc8f48c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2D(nn.Module):\n",
    "    \"\"\"https://github.com/seungjunlee96/Depthwise-Separable-Convolution_Pytorch/blob/master/DepthwiseSeparableConvolution/DepthwiseSeparableConvolution.py\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        depth_multiplier=1,\n",
    "        kernel_size=3,\n",
    "        padding=\"valid\",\n",
    "        bias=False,\n",
    "    ):\n",
    "        super(SeparableConv2D, self).__init__()\n",
    "        self.depthwise = DepthwiseConv2d(\n",
    "            in_channels, depth_multiplier, kernel_size, padding=padding, bias=bias\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DepthwiseConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        depth_multiplier=1,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        out_channels = in_channels * depth_multiplier\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "\n",
    "class EEGNetv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Note: Use this class carefully. It is specificaly adapted with BCI Comp. IV 2a\n",
    "        (22 channels, sample rate of 250, 3 seconds). So it's not a general class.\n",
    "        it is developed only for experiment purposes.\n",
    "    Implemented based on keras/tensorflow implementation found here:\n",
    "        https://github.com/vlawhern/arl-eegmodels\n",
    "    Paper:\n",
    "        https://iopscience.iop.org/article/10.1088/1741-2552/aace8c\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=4,\n",
    "        channels=22,\n",
    "        dropout_rate=0.5,\n",
    "        kernel_length=64,\n",
    "        F1=8,\n",
    "        D=2,\n",
    "        F2=16,\n",
    "    ):\n",
    "        super(EEGNetv2, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_classes, F1, (1, kernel_length), padding=\"valid\", bias=False\n",
    "        )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(F1, False)\n",
    "        self.dwconv2 = DepthwiseConv2d(\n",
    "            in_channels=F1,\n",
    "            depth_multiplier=D,\n",
    "            kernel_size=(channels, 1),\n",
    "            stride=1,\n",
    "            padding=\"valid\",\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.batchnorm2 = nn.BatchNorm2d(2 * F1, False)\n",
    "        # act elu\n",
    "        self.pooling1 = nn.AvgPool2d((1, 4))\n",
    "        # dropout\n",
    "\n",
    "        # Layer 2\n",
    "        self.sepconv2 = SeparableConv2D(2 * F1, F2, 1, (1, 16), padding=\"same\")\n",
    "        self.batchnorm3 = nn.BatchNorm2d(F2, False)\n",
    "        # elu\n",
    "        self.pooling2 = nn.AvgPool2d((1, 8))\n",
    "        # dropout\n",
    "\n",
    "        # FC Layer\n",
    "        self.fc1 = nn.Linear(16 * 111, num_classes)\n",
    "\n",
    "    def _forward_emb(self, x, device=None):\n",
    "        # Layer 1\n",
    "        # print(f\"{x.size()}\")\n",
    "        x = self.conv1(x)\n",
    "        # print(f\"{x.size()}\")\n",
    "        x = self.batchnorm1(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        # print(f\"{x.size()}\")\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.pooling1(x)\n",
    "        if self.training:\n",
    "            x = F.dropout(x, self.dropout_rate)\n",
    "\n",
    "        x = self.sepconv2(x)\n",
    "        # print(f\"{x.size()}\")\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.elu(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.pooling2(x)\n",
    "        # print(f\"{x.size()}\")\n",
    "        if self.training:\n",
    "            x = F.dropout(x, self.dropout_rate)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.reshape((-1, 16 * 111))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, device=None):\n",
    "        x = self._forward_emb(x)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSPNNCls(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        num_features: int = None,\n",
    "        num_bands: int = None,\n",
    "        num_windows: int = 1,\n",
    "        num_labels: int = None,\n",
    "        csp_pow: bool = True,\n",
    "        signal_len: int = None,\n",
    "        mode: str = \"constant\",\n",
    "        dropout_rate=0.5,\n",
    "    ):\n",
    "        super(CSPNNCls, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_features = num_channels if num_features is None else num_features\n",
    "        self.num_bands = num_bands\n",
    "        self.num_windows = num_windows\n",
    "        self.num_labels = num_labels\n",
    "        self.csp_pow = csp_pow\n",
    "        if not self.csp_pow:\n",
    "            self.signal_len = signal_len\n",
    "        self.mode = mode\n",
    "\n",
    "        self.csp_nn = CSPNN(\n",
    "            num_channels=num_channels,\n",
    "            num_features=num_features,\n",
    "            num_bands=num_bands,\n",
    "            num_windows=num_windows,\n",
    "            num_labels=num_labels,\n",
    "            csp_pow=csp_pow,\n",
    "            mode=self.mode,\n",
    "        )\n",
    "\n",
    "        if self.csp_pow:\n",
    "            csp_feature_size = (\n",
    "                self.num_bands * self.num_windows * self.num_labels * self.num_features\n",
    "            )\n",
    "        else:\n",
    "            csp_feature_size = (\n",
    "                self.num_bands\n",
    "                * self.num_windows\n",
    "                * self.num_labels\n",
    "                * self.num_features\n",
    "                * 51\n",
    "            )\n",
    "\n",
    "        self.eegnet = EEGNetv2(\n",
    "            num_classes=self.num_labels,\n",
    "            channels=self.num_channels,\n",
    "            dropout_rate=dropout_rate,\n",
    "            kernel_length=32,\n",
    "            F1=8,\n",
    "            D=2,\n",
    "            F2=16,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        csp = self.csp_nn(x)\n",
    "\n",
    "        # features = csp.reshape(\n",
    "        #     (csp.size()[0] * csp.size()[1], self.num_channels, -1)  # batch * labels\n",
    "        # )\n",
    "        print(csp.size())\n",
    "\n",
    "        # features = csp.reshape(\n",
    "        #     (csp.size()[0], self.num_channels, csp.size()[1], -1)  # batch * labels\n",
    "        # )\n",
    "        # features = features.permute(0, 2, 1, 3)\n",
    "\n",
    "        features = csp\n",
    "\n",
    "        x = self.eegnet(features)\n",
    "\n",
    "        if self.training:\n",
    "            return x, csp\n",
    "        return x, csp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b1cc0-b2a5-4842-a92b-62e3b55e3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CSPNNCls(\n",
    "    num_channels=68,\n",
    "    num_features=68,\n",
    "    num_bands=1,\n",
    "    num_windows=1,\n",
    "    num_labels=2,\n",
    "    csp_pow=False,\n",
    "    signal_len=3584,\n",
    "    mode=\"csp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72275b7-168e-41c6-ac3e-d58d3fc4f381",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e265da0-423a-4b02-aedd-398b89a35522",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    ToTensor(device=\"cpu\"),\n",
    "    ExpandDim(dim=0),\n",
    "    ExpandDim(dim=2),\n",
    "    ExpandDim(dim=2),\n",
    "    LabelToDict(),\n",
    "]\n",
    "compose = Compose(transforms=transforms)\n",
    "\n",
    "ds = ChoDataset(\n",
    "    data_path=\"./cho/\",\n",
    "    patients=[1],\n",
    "    transforms=compose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6b01d-1d7b-4232-8949-0cbe339e6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ff4c7-9728-4d0b-aaea-d274a2b1c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, train_dataset, test_dataset in ds.get_train_test_subsets(with_key=True):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=ChoDataset.collate_fn,\n",
    "        num_workers=1,\n",
    "    )\n",
    "    transforms = [\n",
    "        ToTensor(device=\"cpu\"),  # \"cuda\"),\n",
    "        ExpandDim(dim=0),\n",
    "        Whitening(train_dataloader, whitening_method=\"ZCA\"),\n",
    "        ExpandDim(dim=2),\n",
    "        ExpandDim(dim=2),\n",
    "        LabelToDict(),\n",
    "    ]\n",
    "    train_dataset.transforms = Compose(transforms=transforms)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=ChoDataset.collate_fn,\n",
    "        num_workers=1,  # os.cpu_count(),\n",
    "    )\n",
    "    for signals, labels in train_dataloader:\n",
    "        preds, csp = net(signals)\n",
    "        print(preds.size(), labels[\"label\"].size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5712b-036b-48e5-bd51-55efbaf0cf12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
