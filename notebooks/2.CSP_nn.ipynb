{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd28a1f-35dc-4a3d-bd19-5cfa47be113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "# %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9cc00-af20-4ef0-b39e-0f4544f71222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e81fd4-23ee-4d7a-a8f7-cd1cf92074c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametrization(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the parametrization of a manifold in terms of a Euclidean space\n",
    "\n",
    "    It gives the parametrized matrix through the attribute `B`\n",
    "\n",
    "    To use it, subclass it and implement the method `retraction` and the method `forward`\n",
    "    (and optionally `project`). See the documentation in these methods for details\n",
    "\n",
    "    You can find an example in the file `orthogonal.py` where we implement the Orthogonal\n",
    "    class to optimize over the Stiefel manifold using an arbitrary retraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A, base, mode):\n",
    "        \"\"\"\n",
    "        mode: \"static\" or a tuple such that:\n",
    "                mode[0] == \"dynamic\"\n",
    "                mode[1]: int, K, the number of steps after which we should change\n",
    "                            the basis of the dyn triv\n",
    "                mode[2]: int, M, the number of changes of basis after which we\n",
    "                            should project back onto the manifold the basis.\n",
    "                            This is particularly helpful for small values of K.\n",
    "        \"\"\"\n",
    "        super(Parametrization, self).__init__()\n",
    "        assert mode == \"static\" or (\n",
    "            isinstance(mode, tuple) and len(mode) == 3 and mode[0] == \"dynamic\"\n",
    "        )\n",
    "\n",
    "        self.A = nn.Parameter(A)\n",
    "        self.register_buffer(\"_B\", None)\n",
    "        self.register_buffer(\"base\", base)\n",
    "        # This is necessary, as it will be generated again the first time that self.B is called\n",
    "        # We still need to register the buffer though\n",
    "\n",
    "        if mode == \"static\":\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            self.mode = mode[0]\n",
    "            self.K = mode[1]\n",
    "            self.M = mode[2]\n",
    "            self.k = 0\n",
    "            self.m = 0\n",
    "\n",
    "        # This implements the parametrization trick in a rather slick way.\n",
    "        # We put a hook on A, such that, whenever its gradients are computed, we\n",
    "        #  get rid of self._B so that it has to be recomputed the next time that\n",
    "        #  self.B is accessed\n",
    "        def hook(grad):\n",
    "            nonlocal self\n",
    "            self._B = None\n",
    "\n",
    "        self.A.register_hook(hook)\n",
    "\n",
    "    def rebase(self):\n",
    "        with torch.no_grad():\n",
    "            self.base.data.copy_(self._B.data)\n",
    "            self.A.data.zero_()\n",
    "\n",
    "    # seems we can't use python properties with pytorch modules for now\n",
    "    # see this issue: https://github.com/pytorch/pytorch/issues/49726\n",
    "    # @property\n",
    "    def B(self):\n",
    "        not_B = self._B is None\n",
    "        if not_B or (not self._B.grad_fn and torch.is_grad_enabled()):\n",
    "            self._B = self.retraction(self.A, self.base)\n",
    "            # Just to be safe\n",
    "            self._B.requires_grad_()\n",
    "            # Now self._B it's not a leaf tensor, so we convert it into a leaf\n",
    "            self._B.retain_grad()\n",
    "\n",
    "            # Increment the counters for the dyntriv algorithm if we have generated B\n",
    "            if self.mode == \"dynamic\" and not_B:\n",
    "                if self.k == 0:\n",
    "                    self.rebase()\n",
    "                    # Project the base back to the manifold every M changes of base\n",
    "                    # Increment the counter before as we don't project the first time\n",
    "                    self.m = (self.m + 1) % self.M\n",
    "                    # It's optional to implement this method\n",
    "                    if self.m == 0 and hasattr(self, \"project\"):\n",
    "                        with torch.no_grad():\n",
    "                            self.base = self.project(self.base)\n",
    "                # Change the basis after K optimization steps\n",
    "                # Increment the counter afterwards as we change the basis in the first iteration\n",
    "                if self.K != \"infty\":\n",
    "                    self.k = (self.k + 1) % self.K\n",
    "                else:\n",
    "                    # Make sure that we just update the base once\n",
    "                    if self.k == 0:\n",
    "                        self.k = 1\n",
    "\n",
    "        return self._B\n",
    "\n",
    "    def retraction(self, A, base):\n",
    "        \"\"\"\n",
    "        It computes r_{base}(A).\n",
    "        Notice that A will not always be in the tangent space of our manifold\n",
    "          For this reason, we first have to use A to parametrize the tangent space,\n",
    "          and then compute the retraction\n",
    "        When dealing with Lie groups, raw_A is always projected into the Lie algebra,\n",
    "          as an optimization (cf. Section E in the paper)\n",
    "\n",
    "        more:\n",
    "        -----\n",
    "        A retraction retr_p can be interpreted as a first order\n",
    "        approximation to the exponential map exp_p.\n",
    "            see: chapter 4.1 of Optomization algorithms on matrix manifolds\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def project(self, base):\n",
    "        \"\"\"\n",
    "        This method is OPTIONAL\n",
    "        It returns the projected base back into the manifold\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        It uses the attribute self.B to implement the layer itself (e.g. Linear, CNN, ...)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df9074-a3b7-4ff3-aef8-406f120d139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orthogonal(Parametrization):\n",
    "    \"\"\"Class that implements optimization restricted to the Stiefel manifold\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, initializer_skew, mode, param):\n",
    "        \"\"\"\n",
    "        mode: \"static\" or a tuple such that:\n",
    "                mode[0] == \"dynamic\"\n",
    "                mode[1]: int, K, the number of steps after which we should change the basis of the dyn triv\n",
    "                mode[2]: int, M, the number of changes of basis after which we should project back onto the manifold the basis. This is particularly helpful for small values of K.\n",
    "\n",
    "        param: A parametrization of in terms of skew-symmetyric matrices\n",
    "        \"\"\"\n",
    "        max_size = max(input_size, output_size)\n",
    "        A = torch.empty(max_size, max_size)\n",
    "        base = torch.empty(max_size, max_size)\n",
    "        super(Orthogonal, self).__init__(A, base, mode)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.param = param\n",
    "        self.init_A = initializer_skew\n",
    "        self.init_base = nn.init.eye_\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.init_A(self.A)\n",
    "        self.init_base(self.base)\n",
    "\n",
    "    def retraction(self, A, base):\n",
    "        # This could be any parametrization of a tangent space\n",
    "        A = A.triu(diagonal=1)\n",
    "        A = A - A.t()\n",
    "        B = base.mm(self.param(A))\n",
    "        if self.input_size != self.output_size:\n",
    "            B = B[: self.input_size, : self.output_size]\n",
    "        return B\n",
    "\n",
    "    def project(self, base):\n",
    "        try:\n",
    "            # Compute the projection using the thin SVD decomposition\n",
    "            U, _, V = torch.svd(base, some=True)\n",
    "            return U.mm(V.t())\n",
    "        except RuntimeError:\n",
    "            # If the svd does not converge, fallback to the (thin) QR decomposition\n",
    "            x = base\n",
    "            if base.size(0) < base.size(1):\n",
    "                x = base.t()\n",
    "            ret = torch.qr(x, some=True).Q\n",
    "            if base.size(0) < base.size(1):\n",
    "                ret = ret.t()\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29458786-2822-4961-a576-6d1ceb545b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_diag_(A, diag):\n",
    "    n = A.size(0)\n",
    "    diag_z = torch.zeros(n - 1)\n",
    "    diag_z[::2] = diag\n",
    "    A_init = torch.diag(diag_z, diagonal=1)\n",
    "    A_init = A_init - A_init.T\n",
    "    with torch.no_grad():\n",
    "        A.copy_(A_init)\n",
    "        return A\n",
    "\n",
    "\n",
    "def cayley_init_(A):\n",
    "    size = A.size(0) // 2\n",
    "    diag = A.new(size).uniform_(0.0, np.pi / 2.0)\n",
    "    diag = -torch.sqrt((1.0 - torch.cos(diag)) / (1.0 + torch.cos(diag)))\n",
    "    return create_diag_(A, diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac56ed2-102b-440f-8d04-a80598966a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cayley_map(X):\n",
    "    n = X.size(0)\n",
    "    Id = torch.eye(n, dtype=X.dtype, device=X.device)\n",
    "    return torch.linalg.solve(Id - X, Id + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2e4bc-1ab5-49f4-b86d-66e9c8b96712",
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = Orthogonal(22, 11, cayley_init_, \"static\", cayley_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bd5a9-74b0-43c9-b182-b64b0ec2517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho.B().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3579c4-f08d-4b92-988d-ddd661720bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (True, False) columns are orthogonal\n",
    "# if (False, True) rows are orthogonal\n",
    "B = ortho.B()\n",
    "torch.allclose(B.T @ B, torch.eye(B.size(1)), atol=1e-6), torch.allclose(\n",
    "    B @ B.T, torch.eye(B.size(0)), atol=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0fc2c-17e3-4813-bb19-5d62def22535",
   "metadata": {},
   "outputs": [],
   "source": [
    "B[:, 0] @ B[:, 1], B[:, 0] @ B[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9529d-edd2-4e99-98c8-5140e09d616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.utils import _pair, _single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6890cc8-e86d-45fc-9136-e8f1279343c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSP(Orthogonal):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        num_features: int = None,\n",
    "        # stride: _size_2_t = 1,\n",
    "        # padding: Union[str, _size_2_t] = 0,\n",
    "        # dilation: _size_2_t = 1,\n",
    "        # groups: int = 1,\n",
    "        # bias: bool = True,\n",
    "        # padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        self.num_channels = num_channels\n",
    "        self.num_features = num_channels if num_features is None else num_features\n",
    "        super(CSP, self).__init__(\n",
    "            self.num_channels, self.num_features, cayley_init_, \"static\", cayley_map\n",
    "        )\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor):\n",
    "        weight = torch.permute(weight, (1, 0))\n",
    "        weight = weight.unsqueeze(2).unsqueeze(1)\n",
    "\n",
    "        assert weight.size()[0] == self.output_size\n",
    "        assert weight.size()[2] == self.input_size\n",
    "        assert torch.allclose(\n",
    "            weight[0, :, :, :].squeeze() @ weight[0, :, :, :].squeeze(),\n",
    "            torch.tensor(1.0),\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            weight[0, :, :, :].squeeze() @ weight[1, :, :, :].squeeze(),\n",
    "            torch.tensor(0.0),\n",
    "            atol=1e-6,\n",
    "        )\n",
    "        return F.conv2d(input, weight, None, _single(1), _single(0), _single(1), 1)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.B())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e086b0-e664-4beb-ab64-7836ac53b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csp = CSP(22, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cba922-40f3-4e74-b407-583ae21ab52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = torch.empty(1, 1, 22, 769)\n",
    "csp(inpt).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab6981-e1b3-44b6-8c7a-cfd0fd978345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc05472-5750-44ed-90e0-a76b961e818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSPNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        num_features: int = None,\n",
    "        num_bands: int = None,\n",
    "        num_windows: int = 1,\n",
    "        num_labels: int = None,\n",
    "        mode: str = \"constant\",\n",
    "    ):\n",
    "        assert (\n",
    "            num_bands is not None\n",
    "        ), f\"expected 'num_bands' to be int but got {num_bands}\"\n",
    "        assert (\n",
    "            num_labels is not None\n",
    "        ), f\"expected 'num_labels' to be int but got {num_labels}\"\n",
    "\n",
    "        super(CSPNN, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_features = num_channels if num_features is None else num_features\n",
    "        self.num_bands = num_bands\n",
    "        self.num_windows = num_windows\n",
    "        self.num_labels = num_labels\n",
    "        self.mode = mode\n",
    "\n",
    "        self.template = \"label-{}_band-{}_window-{}\"\n",
    "\n",
    "        self.cspw = nn.ModuleDict(\n",
    "            [\n",
    "                [\n",
    "                    self.template.format(*i),\n",
    "                    nn.Conv2d(\n",
    "                        1,\n",
    "                        self.num_features,\n",
    "                        (self.num_channels, 1),\n",
    "                        padding=0,\n",
    "                        bias=False,\n",
    "                    )\n",
    "                    if self.mode == \"constant\"\n",
    "                    else CSP(self.num_channels, self.num_features),\n",
    "                ]\n",
    "                for i in itertools.product(\n",
    "                    range(self.num_labels),\n",
    "                    range(self.num_bands),\n",
    "                    range(self.num_windows),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _csp_post_projection(self, x):\n",
    "        # [filters x signals] . [signals x filters] = [filters x filters]\n",
    "        x = torch.matmul(x, x.transpose(3, 2))  # [batch, band, filters, filters]\n",
    "        print(x.size())\n",
    "\n",
    "        num = torch.diagonal(x, dim1=2, dim2=3)  # [batch, band, filters]\n",
    "        print(f\"{num.size()=}\")\n",
    "        den = torch.sum(num, dim=2).unsqueeze(2)  # [batch, band, 1]\n",
    "        print(f\"{den.size()=}\")\n",
    "\n",
    "        csp = torch.log(torch.div(num, den))  # [batch, band, filters]\n",
    "        return csp\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch, channel, window, band, signal] to [batch, window, band, channel, signal]\n",
    "        x = x.permute(0, 2, 3, 1, 4)\n",
    "\n",
    "        x = torch.vstack(\n",
    "            [\n",
    "                self.cspw[self.template.format(*i)](\n",
    "                    x[:, i[2], i[1], :, :].unsqueeze(1)\n",
    "                ).unsqueeze(0)\n",
    "                for i in itertools.product(\n",
    "                    range(self.num_labels),\n",
    "                    range(self.num_bands),\n",
    "                    range(self.num_windows),\n",
    "                )\n",
    "            ]\n",
    "        ).squeeze()  # [batch, band * window, filters(kernels), signal]\n",
    "        print(f\"{x.size() = }\")\n",
    "\n",
    "        if len(x.size()) <= 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        else:\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            print(f\"{x.size() = }\")\n",
    "\n",
    "        csp = self._csp_post_projection(x)\n",
    "        print(f\"{csp.size()=}\")\n",
    "        return torch.flatten(csp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042500c9-bff3-4ebc-8898-f977fb1a5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csplayer = CSPNN(22, 11, 16, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd125a-9e04-4080-a0f6-17a5a01b5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = torch.empty((10, 22, 1, 16, 769)).random_(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bf410-1a04-44cc-a1f7-b876b25e0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "csplayer(signals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
